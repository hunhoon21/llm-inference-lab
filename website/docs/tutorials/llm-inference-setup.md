---
sidebar_position: 2
---

# LLM ì¶”ë¡  ì„œë¹„ìŠ¤ êµ¬ì¶•

GPU ì¸ìŠ¤í„´ìŠ¤ì— vLLMì„ ì„¤ì¹˜í•˜ê³  LLM ì¶”ë¡  ì„œë¹„ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ

- GPU í• ë‹¹ëŸ‰ ìš”ì²­ ë° ìŠ¹ì¸
- GPU ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì„¤ì •
- vLLM ì„¤ì¹˜ ë° êµ¬ì„±
- FastAPI ê¸°ë°˜ ì¶”ë¡  ì„œë²„ êµ¬ì¶•
- ì™¸ë¶€ì—ì„œ API ì ‘ê·¼ í…ŒìŠ¤íŠ¸

## ğŸ“‹ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- [GCP ê¸°ë³¸ í™˜ê²½ êµ¬ì¶•](gcp-basic-setup.md) ì™„ë£Œ
- ê¸°ë³¸ì ì¸ Python ì§€ì‹
- Linux ëª…ë ¹ì–´ ì‚¬ìš©ë²•

## ğŸš€ 1. GPU í• ë‹¹ëŸ‰ ìš”ì²­

### í• ë‹¹ëŸ‰ í˜„í™© í™•ì¸

```bash
# í˜„ì¬ í• ë‹¹ëŸ‰ í™•ì¸
gcloud compute project-info describe --project=$PROJECT_ID
```

### GPU í• ë‹¹ëŸ‰ ìš”ì²­

1. [í• ë‹¹ëŸ‰ í˜ì´ì§€](https://console.cloud.google.com/iam-admin/quotas) ì ‘ì†
2. "Compute Engine API" í•„í„° ì ìš©
3. "GPUs (all regions)" ê²€ìƒ‰
4. NVIDIA L4 ë˜ëŠ” T4 GPU ì„ íƒ
5. í• ë‹¹ëŸ‰ ì¦ê°€ ìš”ì²­

### ğŸ† ë¹ ë¥¸ ìŠ¹ì¸ì„ ìœ„í•œ ì‚¬ìœ  ì‘ì„±ë²•

:::tip 1ë¶„ ë‚´ ìŠ¹ì¸ë°›ì€ ì‹¤ì œ ì‚¬ìœ 
```
We are requesting additional GPU quota (NVIDIA L4) to support our Large Language Model (LLM) research and development.
The GPU resources will be used for model inference, benchmarking, and fine-tuning experiments in a controlled environment.
This is part of our ongoing R&D project to evaluate performance and scalability of LLMs on Google Cloud infrastructure.
```

**í•µì‹¬ ìš”ì†Œ:**
- êµ¬ì²´ì ì¸ ê¸°ìˆ  ìš©ë„ ëª…ì‹œ
- ì—°êµ¬/ê°œë°œ ëª©ì  ê°•ì¡°  
- ì „ë¬¸ì ì´ê³  ëª…í™•í•œ ì˜ë¬¸ ì‘ì„±
- GCP ì¸í”„ë¼ í™œìš© ì˜ì§€ í‘œí˜„
:::

## ğŸ–¥ï¸ 2. GPU ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

### Deep Learning VM ì´ë¯¸ì§€ í™•ì¸

```bash
# ì‚¬ìš© ê°€ëŠ¥í•œ PyTorch ì´ë¯¸ì§€ ì¡°íšŒ
gcloud compute images list \
    --project=deeplearning-platform-release \
    --filter="family:pytorch*" \
    --limit=10

# ìµœì‹  ì´ë¯¸ì§€ ì„ íƒ (2025ë…„ 8ì›” ê¸°ì¤€)
IMAGE_FAMILY="pytorch-2-7-cu128-ubuntu-2204-nvidia-570"
IMAGE_PROJECT="deeplearning-platform-release"
```

### GPU ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
<TabItem value="l4" label="L4 GPU (ê¶Œì¥)">

```bash
# L4 GPU ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
gcloud compute instances create llm-inference-server \
    --zone=us-central1-a \
    --machine-type=g2-standard-4 \
    --accelerator=type=nvidia-l4,count=1 \
    --image-family=$IMAGE_FAMILY \
    --image-project=$IMAGE_PROJECT \
    --boot-disk-size=100GB \
    --boot-disk-type=pd-ssd \
    --maintenance-policy=TERMINATE \
    --restart-on-failure \
    --tags=llm-server
```

</TabItem>
<TabItem value="t4" label="T4 GPU (ë¹„ìš© ì ˆì•½)">

```bash
# T4 GPU ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
gcloud compute instances create llm-inference-server \
    --zone=us-central1-f \
    --machine-type=n1-standard-4 \
    --accelerator=type=nvidia-tesla-t4,count=1 \
    --image-family=$IMAGE_FAMILY \
    --image-project=$IMAGE_PROJECT \
    --boot-disk-size=100GB \
    --boot-disk-type=pd-ssd \
    --maintenance-policy=TERMINATE \
    --restart-on-failure \
    --tags=llm-server
```

</TabItem>
</Tabs>

### ë°©í™”ë²½ ì„¤ì •

```bash
# LLM API ì„œë²„ìš© í¬íŠ¸ ê°œë°©
gcloud compute firewall-rules create allow-llm-api \
    --allow tcp:8000 \
    --source-ranges 0.0.0.0/0 \
    --target-tags llm-server \
    --description="Allow LLM inference API access"

# ëª¨ë‹ˆí„°ë§ìš© í¬íŠ¸ (ì„ íƒì‚¬í•­)
gcloud compute firewall-rules create allow-llm-monitoring \
    --allow tcp:8080 \
    --source-ranges 0.0.0.0/0 \
    --target-tags llm-server \
    --description="Allow LLM monitoring dashboard"
```

## ğŸ¤– 3. vLLM ì„¤ì¹˜ ë° ì„¤ì •

### ì¸ìŠ¤í„´ìŠ¤ ì ‘ì†

```bash
# GPU ì¸ìŠ¤í„´ìŠ¤ì— SSH ì ‘ì†
gcloud compute ssh llm-inference-server \
    --zone=us-central1-a \
    --ssh-key-file ~/.ssh/gcp-key
```

### í™˜ê²½ í™•ì¸

```bash
# GPU í™•ì¸
nvidia-smi

# Python í™˜ê²½ í™•ì¸
python3 --version
pip3 --version

# CUDA í™•ì¸
nvcc --version
```

### vLLM ì„¤ì¹˜

```bash
# Python ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv vllm-env
source vllm-env/bin/activate

# vLLM ì„¤ì¹˜ (CUDA ì§€ì›)
pip install vllm

# ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install fastapi uvicorn[standard] requests

# ì„¤ì¹˜ í™•ì¸
python -c "import vllm; print(vllm.__version__)"
```

## ğŸ“¡ 4. FastAPI ì„œë²„ êµ¬ì¶•

### ì„œë²„ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

```python title="vllm_server.py"
#!/usr/bin/env python3
"""
vLLM FastAPI ì„œë²„
HuggingFace ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì¶”ë¡  API ì œê³µ
"""

import asyncio
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from vllm import LLM, SamplingParams
from typing import List, Optional
import uvicorn
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="LLM Inference API", version="1.0.0")

# ì „ì—­ ë³€ìˆ˜
llm = None
model_name = "microsoft/DialoGPT-medium"  # ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ì‹œì‘

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 100
    temperature: float = 0.7
    top_p: float = 0.9
    stop: Optional[List[str]] = None

class GenerateResponse(BaseModel):
    text: str
    prompt: str
    model: str

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global llm, model_name
    try:
        logger.info(f"Loading model: {model_name}")
        llm = LLM(
            model=model_name,
            tensor_parallel_size=1,  # GPU 1ê°œ ì‚¬ìš©
            dtype="half",  # ë©”ëª¨ë¦¬ ì ˆì•½
        )
        logger.info("Model loaded successfully")
    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        raise

@app.get("/")
async def root():
    return {"message": "LLM Inference API is running", "model": model_name}

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy", "model_loaded": llm is not None}

@app.post("/generate", response_model=GenerateResponse)
async def generate_text(request: GenerateRequest):
    """í…ìŠ¤íŠ¸ ìƒì„± API"""
    if llm is None:
        raise HTTPException(status_code=500, detail="Model not loaded")
    
    try:
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
        sampling_params = SamplingParams(
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
            stop=request.stop
        )
        
        # ì¶”ë¡  ì‹¤í–‰
        outputs = llm.generate([request.prompt], sampling_params)
        generated_text = outputs[0].outputs[0].text
        
        return GenerateResponse(
            text=generated_text,
            prompt=request.prompt,
            model=model_name
        )
        
    except Exception as e:
        logger.error(f"Generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
```

### ì„œë²„ ì‹¤í–‰

```bash
# ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„œë²„ ì‹¤í–‰
nohup python vllm_server.py > vllm_server.log 2>&1 &

# ì„œë²„ ìƒíƒœ í™•ì¸ (ëª¨ë¸ ë¡œë”© ëŒ€ê¸°)
sleep 30
curl http://localhost:8000/health
```

## ğŸ§ª 5. API í…ŒìŠ¤íŠ¸

### ë¡œì»¬ í…ŒìŠ¤íŠ¸

```bash
# í—¬ìŠ¤ ì²´í¬
curl http://localhost:8000/

# í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/generate" \
     -H "Content-Type: application/json" \
     -d '{
       "prompt": "The future of AI is",
       "max_tokens": 50,
       "temperature": 0.7
     }'
```

### ì™¸ë¶€ì—ì„œ ì ‘ê·¼ í…ŒìŠ¤íŠ¸

```bash
# ë¡œì»¬ ë¨¸ì‹ ì—ì„œ ì‹¤í–‰
# GPU ì¸ìŠ¤í„´ìŠ¤ì˜ ì™¸ë¶€ IP í™•ì¸
EXTERNAL_IP=$(gcloud compute instances describe llm-inference-server \
    --zone=us-central1-a \
    --format='get(networkInterfaces[0].accessConfigs[0].natIP)')
echo "LLM Server IP: $EXTERNAL_IP"

# ì™¸ë¶€ì—ì„œ API í˜¸ì¶œ
curl -X POST "http://$EXTERNAL_IP:8000/generate" \
     -H "Content-Type: application/json" \
     -d '{
       "prompt": "Explain machine learning in simple terms",
       "max_tokens": 100
     }'
```

## ğŸ“Š 6. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### GPU ì‚¬ìš©ë¥  í™•ì¸

```bash
# ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
while true; do
    echo "$(date): $(nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits)"
    sleep 5
done > gpu_memory.log
```

### ì„œë²„ ë¡œê·¸ í™•ì¸

```bash
# ì„œë²„ ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
tail -f vllm_server.log

# ì˜¤ë¥˜ ë¡œê·¸ ê²€ìƒ‰
grep -i error vllm_server.log
```

## ğŸ”§ 7. ì„±ëŠ¥ ìµœì í™”

### vLLM íŠœë‹ ì˜µì…˜

```python
# ìµœì í™”ëœ ì„¤ì • ì˜ˆì‹œ
llm = LLM(
    model=model_name,
    tensor_parallel_size=1,
    dtype="half",                    # ë©”ëª¨ë¦¬ ì ˆì•½
    max_model_len=2048,             # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì œí•œ
    gpu_memory_utilization=0.8,     # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¡°ì •
    swap_space=4,                   # ìŠ¤ì™‘ ê³µê°„ (GB)
)
```

### ì„œë¹„ìŠ¤ ìë™ ì¬ì‹œì‘

```bash
# Systemd ì„œë¹„ìŠ¤ ìƒì„±
sudo tee /etc/systemd/system/vllm-server.service << EOF
[Unit]
Description=vLLM Inference Server
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=/home/$USER
Environment=PATH=/home/$USER/vllm-env/bin
ExecStart=/home/$USER/vllm-env/bin/python /home/$USER/vllm_server.py
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
EOF

# ì„œë¹„ìŠ¤ í™œì„±í™”
sudo systemctl enable vllm-server
sudo systemctl start vllm-server
sudo systemctl status vllm-server
```

## ğŸ’° 8. ë¹„ìš© ê´€ë¦¬

### ì˜ˆìƒ ë¹„ìš© (ì›” ê¸°ì¤€)

```
GPU ì¸ìŠ¤í„´ìŠ¤ ë¹„ìš© (24ì‹œê°„ ìš´ì˜):
- L4 GPU + g2-standard-4: ~$400-500/ì›”
- T4 GPU + n1-standard-4: ~$300-400/ì›”

ë¹„ìš© ì ˆì•½ íŒ:
- ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ì§€
- Preemptible ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© (50-70% í• ì¸)
- ì ì ˆí•œ GPU íƒ€ì… ì„ íƒ
```

### ë¦¬ì†ŒìŠ¤ ì •ë¦¬

```bash
# ì¸ìŠ¤í„´ìŠ¤ ì¤‘ì§€ (ê³¼ê¸ˆ ì¤‘ë‹¨)
gcloud compute instances stop llm-inference-server --zone=us-central1-a

# ì¸ìŠ¤í„´ìŠ¤ ì‚­ì œ
gcloud compute instances delete llm-inference-server --zone=us-central1-a

# ë°©í™”ë²½ ê·œì¹™ ì‚­ì œ
gcloud compute firewall-rules delete allow-llm-api
gcloud compute firewall-rules delete allow-llm-monitoring
```

## âœ… ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] GPU í• ë‹¹ëŸ‰ ìŠ¹ì¸
- [ ] GPU ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
- [ ] vLLM ì„¤ì¹˜
- [ ] FastAPI ì„œë²„ êµ¬ì¶•
- [ ] ë¡œì»¬ API í…ŒìŠ¤íŠ¸ ì„±ê³µ
- [ ] ì™¸ë¶€ API ì ‘ê·¼ ì„±ê³µ
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„

LLM ì¶”ë¡  ì„œë¹„ìŠ¤ êµ¬ì¶•ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ [í´ë¼ì´ì–¸íŠ¸ ì—°ë™](client-integration.md)ìœ¼ë¡œ ë„˜ì–´ê°€ì„¸ìš”.

---

:::warning ì£¼ì˜ì‚¬í•­
GPU ì¸ìŠ¤í„´ìŠ¤ëŠ” ë†’ì€ ë¹„ìš©ì´ ë°œìƒí•©ë‹ˆë‹¤. ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•ŒëŠ” ë°˜ë“œì‹œ ì¤‘ì§€í•˜ì„¸ìš”!
:::