---
sidebar_position: 3
---

# ë¬¸ì œ í•´ê²°

LLM ì¶”ë¡  ì„œë¹„ìŠ¤ ìš´ì˜ ì¤‘ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì£¼ìš” ë¬¸ì œë“¤ê³¼ í•´ê²° ë°©ë²•ì…ë‹ˆë‹¤.

## ğŸš¨ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ

### 1. GPU í• ë‹¹ëŸ‰ ë¬¸ì œ

**ì¦ìƒ**: `Quota exceeded` ì˜¤ë¥˜

**í•´ê²°ë²•**:
- [GCP í• ë‹¹ëŸ‰ í˜ì´ì§€](https://console.cloud.google.com/iam-admin/quotas)ì—ì„œ GPU í• ë‹¹ëŸ‰ ìš”ì²­
- êµ¬ì²´ì ì¸ ì‚¬ìœ  ì‘ì„± (ì—°êµ¬/ê°œë°œ ëª©ì )

### 2. SSH ì ‘ì† ì‹¤íŒ¨

**ì¦ìƒ**: ì—°ê²° ê±°ë¶€

**í•´ê²°ë²•**:
```bash
# ë°©í™”ë²½ ê·œì¹™ í™•ì¸
gcloud compute firewall-rules list | grep ssh

# SSH í‚¤ ê¶Œí•œ í™•ì¸
chmod 400 ~/.ssh/gcp-key
```

### 3. vLLM ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ**: `CUDA out of memory`

**í•´ê²°ë²•**:
```python
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì´ê¸°
LLM(
    model="model-name",
    dtype="half",                    # FP16 ì‚¬ìš©
    gpu_memory_utilization=0.7,      # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¤„ì´ê¸°
    max_model_len=1024,              # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¤„ì´ê¸°
)
```

### 4. API ì ‘ê·¼ ë¶ˆê°€

**ì¦ìƒ**: ì™¸ë¶€ì—ì„œ API í˜¸ì¶œ ì‹¤íŒ¨

**í•´ê²°ë²•**:
```bash
# ë°©í™”ë²½ í™•ì¸
gcloud compute firewall-rules describe allow-llm-api

# ì¸ìŠ¤í„´ìŠ¤ íƒœê·¸ í™•ì¸
gcloud compute instances describe llm-inference-server \
    --zone=us-central1-a \
    --format="value(tags.items)"
```

## ğŸ”§ ë¹ ë¥¸ í•´ê²°ì±…

### ì„œë²„ ì¬ì‹œì‘

```bash
# ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
pkill -f vllm_server.py

# ì„œë²„ ì¬ì‹œì‘
nohup python vllm_server.py > vllm_server.log 2>&1 &
```

### ë¡œê·¸ í™•ì¸

```bash
# ì„œë²„ ë¡œê·¸
tail -f vllm_server.log

# GPU ìƒíƒœ
nvidia-smi

# API í…ŒìŠ¤íŠ¸
curl http://localhost:8000/health
```

ë” ìì„¸í•œ ë¬¸ì œ í•´ê²° ë°©ë²•ì€ [GitHub Issues](https://github.com/hunhoon21/llm-inference-lab/issues)ì—ì„œ í™•ì¸í•˜ì„¸ìš”.