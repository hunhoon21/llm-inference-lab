# 2025-08-14: GCPì—ì„œ LLM ì¶”ë¡  ì„œë¹„ìŠ¤ êµ¬ì¶•

## ğŸ“‹ ì˜¤ëŠ˜ì˜ ëª©í‘œ
GCP Compute Engineì— GPU ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  vLLMì„ ì‚¬ìš©í•˜ì—¬ LLM ì¶”ë¡  ì„œë¹„ìŠ¤ë¥¼ êµ¬ì¶•, ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ì™€ ì—°ë™ í…ŒìŠ¤íŠ¸

## ğŸ¯ í”„ë¡œì íŠ¸ í˜„í™©
- **ì´ì „ ë‹¨ê³„**: GCP ê¸°ë³¸ í™˜ê²½ êµ¬ì¶• ì™„ë£Œ (2025-08-11)
- **í˜„ì¬ ëª©í‘œ**: ì‹¤ì œ LLM ì¶”ë¡  ì„œë¹„ìŠ¤ êµ¬ì¶•
- **ìµœì¢… ëª©í‘œ**: ì•ˆì •ì ì¸ ì¶”ë¡  API ì„œë¹„ìŠ¤ ë° í´ë¼ì´ì–¸íŠ¸ ì—°ë™

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ì„¤ê³„

### ì‹œìŠ¤í…œ êµ¬ì„±ë„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Local Client  â”‚    â”‚  GCP Compute    â”‚    â”‚   Model Hub     â”‚
â”‚                 â”‚    â”‚   (GPU)         â”‚    â”‚  (HuggingFace)  â”‚
â”‚ â€¢ llm_client.py â”‚â”€â”€â”€â–¶â”‚ â€¢ vLLM Server   â”‚â—€â”€â”€â”€â”‚ â€¢ Llama Models  â”‚
â”‚ â€¢ Batch Test    â”‚    â”‚ â€¢ FastAPI       â”‚    â”‚ â€¢ Mistral       â”‚
â”‚ â€¢ Performance   â”‚    â”‚ â€¢ NVIDIA GPU    â”‚    â”‚ â€¢ CodeLlama     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í•µì‹¬ êµ¬ì„±ìš”ì†Œ
- **GPU ì¸ìŠ¤í„´ìŠ¤**: NVIDIA L4/T4 GPUê°€ í¬í•¨ëœ Compute Engine
- **vLLM ì„œë²„**: ê³ ì„±ëŠ¥ LLM ì¶”ë¡  ì—”ì§„
- **FastAPI**: RESTful API ì¸í„°í˜ì´ìŠ¤
- **ëª¨ë¸**: HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ì˜¤í”ˆì†ŒìŠ¤ LLM
- **í´ë¼ì´ì–¸íŠ¸**: ê¸°ì¡´ ê°œë°œëœ Python í´ë¼ì´ì–¸íŠ¸

## ğŸš€ GPU ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •

### 1. GPU í• ë‹¹ëŸ‰ ìš”ì²­
```bash
# í˜„ì¬ í• ë‹¹ëŸ‰ í™•ì¸
gcloud compute project-info describe --project=$PROJECT_ID

# GPU í• ë‹¹ëŸ‰ ìš”ì²­ (ì›¹ ì½˜ì†”ì—ì„œ ìˆ˜í–‰)
# 1. https://console.cloud.google.com/iam-admin/quotas ì ‘ì†
# 2. "Compute Engine API" í•„í„°
# 3. "GPUs (all regions)" ê²€ìƒ‰
# 4. ìš”ì²­í•  GPU ìœ í˜•ê³¼ ìˆ˜ëŸ‰ ì„ íƒ:
#    - NVIDIA_L4: 1ê°œ (ê¶Œì¥)
#    - NVIDIA_T4: 1ê°œ (ëŒ€ì•ˆ)
# 5. ì¦ê°€ ìš”ì²­ ì œì¶œ
```

#### ğŸ’¡ GPU í• ë‹¹ëŸ‰ ìŠ¹ì¸ íŒ
```
ìŠ¹ì¸ ê³¼ì •:
- ì¼ë°˜ì ìœ¼ë¡œ 24-48ì‹œê°„ ì†Œìš”
- ì‹ ê·œ ê³„ì •ì˜ ê²½ìš° ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ
- ìš”ì²­ ì‚¬ìœ ì— "ë¨¸ì‹ ëŸ¬ë‹ ì—°êµ¬/ê°œë°œ" ëª…ì‹œ
- ì˜ˆìƒ ì‚¬ìš© ì‹œê°„ê³¼ ëª©ì  êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ 
```

### 2. Deep Learning VM ì´ë¯¸ì§€ í™•ì¸
```bash
# ì‚¬ìš© ê°€ëŠ¥í•œ Deep Learning VM ì´ë¯¸ì§€ ì¡°íšŒ
gcloud compute images list \
    --project=deeplearning-platform-release \
    --filter="family:pytorch-latest-gpu" \
    --limit=5

# PyTorch + CUDAê°€ ì‚¬ì „ ì„¤ì¹˜ëœ ì´ë¯¸ì§€ ì„ íƒ
IMAGE_FAMILY="pytorch-latest-gpu"
IMAGE_PROJECT="deeplearning-platform-release"
```

### 3. GPU ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
```bash
# L4 GPU ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ê¶Œì¥)
gcloud compute instances create llm-inference-server \
    --zone=us-central1-a \
    --machine-type=g2-standard-4 \
    --accelerator=type=nvidia-l4,count=1 \
    --image-family=$IMAGE_FAMILY \
    --image-project=$IMAGE_PROJECT \
    --boot-disk-size=100GB \
    --boot-disk-type=pd-ssd \
    --maintenance-policy=TERMINATE \
    --restart-on-failure \
    --tags=llm-server

# ëŒ€ì•ˆ: T4 GPU ì¸ìŠ¤í„´ìŠ¤ (ë¹„ìš© ì ˆì•½)
gcloud compute instances create llm-inference-server \
    --zone=us-central1-f \
    --machine-type=n1-standard-4 \
    --accelerator=type=nvidia-tesla-t4,count=1 \
    --image-family=$IMAGE_FAMILY \
    --image-project=$IMAGE_PROJECT \
    --boot-disk-size=100GB \
    --boot-disk-type=pd-ssd \
    --maintenance-policy=TERMINATE \
    --restart-on-failure \
    --tags=llm-server
```

### 4. ë°©í™”ë²½ ê·œì¹™ ì„¤ì •
```bash
# LLM API ì„œë²„ìš© í¬íŠ¸ ê°œë°©
gcloud compute firewall-rules create allow-llm-api \
    --allow tcp:8000 \
    --source-ranges 0.0.0.0/0 \
    --target-tags llm-server \
    --description="Allow LLM inference API access"

# ëª¨ë‹ˆí„°ë§ìš© í¬íŠ¸ (ì„ íƒì‚¬í•­)
gcloud compute firewall-rules create allow-llm-monitoring \
    --allow tcp:8080 \
    --source-ranges 0.0.0.0/0 \
    --target-tags llm-server \
    --description="Allow LLM monitoring dashboard"
```

## ğŸ¤– vLLM ì„¤ì¹˜ ë° êµ¬ì„±

### 1. ì¸ìŠ¤í„´ìŠ¤ ì ‘ì† ë° í™˜ê²½ í™•ì¸
```bash
# GPU ì¸ìŠ¤í„´ìŠ¤ì— SSH ì ‘ì†
gcloud compute ssh llm-inference-server --zone=us-central1-a

# GPU í™•ì¸
nvidia-smi

# Python í™˜ê²½ í™•ì¸
python3 --version
pip3 --version

# CUDA í™•ì¸
nvcc --version
```

### 2. vLLM ì„¤ì¹˜
```bash
# ì¸ìŠ¤í„´ìŠ¤ ë‚´ë¶€ì—ì„œ ì‹¤í–‰
# Python ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv vllm-env
source vllm-env/bin/activate

# vLLM ì„¤ì¹˜ (CUDA ì§€ì›)
pip install vllm

# ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install fastapi uvicorn[standard] requests

# ì„¤ì¹˜ í™•ì¸
python -c "import vllm; print(vllm.__version__)"
```

### 3. ê°„ë‹¨í•œ vLLM ì„œë²„ êµ¬ì¶•
```bash
# ì„œë²„ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > vllm_server.py << 'EOF'
#!/usr/bin/env python3
"""
vLLM FastAPI ì„œë²„
HuggingFace ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì¶”ë¡  API ì œê³µ
"""

import asyncio
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from vllm import LLM, SamplingParams
from typing import List, Optional
import uvicorn
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="LLM Inference API", version="1.0.0")

# ì „ì—­ ë³€ìˆ˜
llm = None
model_name = "microsoft/DialoGPT-medium"  # ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ì‹œì‘

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 100
    temperature: float = 0.7
    top_p: float = 0.9
    stop: Optional[List[str]] = None

class GenerateResponse(BaseModel):
    text: str
    prompt: str
    model: str

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global llm, model_name
    try:
        logger.info(f"Loading model: {model_name}")
        llm = LLM(
            model=model_name,
            tensor_parallel_size=1,  # GPU 1ê°œ ì‚¬ìš©
            dtype="half",  # ë©”ëª¨ë¦¬ ì ˆì•½
        )
        logger.info("Model loaded successfully")
    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        raise

@app.get("/")
async def root():
    return {"message": "LLM Inference API is running", "model": model_name}

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy", "model_loaded": llm is not None}

@app.post("/generate", response_model=GenerateResponse)
async def generate_text(request: GenerateRequest):
    """í…ìŠ¤íŠ¸ ìƒì„± API"""
    if llm is None:
        raise HTTPException(status_code=500, detail="Model not loaded")
    
    try:
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
        sampling_params = SamplingParams(
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
            stop=request.stop
        )
        
        # ì¶”ë¡  ì‹¤í–‰
        outputs = llm.generate([request.prompt], sampling_params)
        generated_text = outputs[0].outputs[0].text
        
        return GenerateResponse(
            text=generated_text,
            prompt=request.prompt,
            model=model_name
        )
        
    except Exception as e:
        logger.error(f"Generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
EOF

chmod +x vllm_server.py
```

### 4. ì„œë²„ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸
```bash
# ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„œë²„ ì‹¤í–‰
nohup python vllm_server.py > vllm_server.log 2>&1 &

# ì„œë²„ ìƒíƒœ í™•ì¸
sleep 30  # ëª¨ë¸ ë¡œë”© ëŒ€ê¸°
curl http://localhost:8000/health

# ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/generate" \
     -H "Content-Type: application/json" \
     -d '{
       "prompt": "The future of AI is",
       "max_tokens": 50,
       "temperature": 0.7
     }'
```

## ğŸ”— í´ë¼ì´ì–¸íŠ¸ ì—°ë™ ë° í…ŒìŠ¤íŠ¸

### 1. ì™¸ë¶€ IP í™•ì¸ ë° í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
```bash
# ë¡œì»¬ ë¨¸ì‹ ì—ì„œ ì‹¤í–‰
# GPU ì¸ìŠ¤í„´ìŠ¤ì˜ ì™¸ë¶€ IP í™•ì¸
EXTERNAL_IP=$(gcloud compute instances describe llm-inference-server \
    --zone=us-central1-a \
    --format='get(networkInterfaces[0].accessConfigs[0].natIP)')
echo "LLM Server IP: $EXTERNAL_IP"
```

### 2. ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ë¡œ ì—°ë™ í…ŒìŠ¤íŠ¸
```bash
# ë¡œì»¬ ë¨¸ì‹ ì—ì„œ ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
cd client/

# ì„œë²„ URL ì—…ë°ì´íŠ¸
python llm_client.py \
    --base-url "http://$EXTERNAL_IP:8000" \
    --single \
    --prompt "Explain machine learning in simple terms"

# ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python llm_client.py \
    --base-url "http://$EXTERNAL_IP:8000" \
    --batch \
    --input example_prompts.txt \
    --output results.csv \
    --concurrent 2
```

### 3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
```bash
# ëŒ€ê¸°ì‹œê°„ ë° ì²˜ë¦¬ëŸ‰ ì¸¡ì •
python llm_client.py \
    --base-url "http://$EXTERNAL_IP:8000" \
    --benchmark \
    --requests 10 \
    --concurrent 3 \
    --prompt "Write a short story about AI"

# ê²°ê³¼ ë¶„ì„
# - í‰ê·  ëŒ€ê¸°ì‹œê°„
# - ì´ˆë‹¹ ìš”ì²­ ì²˜ë¦¬ëŸ‰ (RPS)
# - ì´ˆë‹¹ í† í° ìƒì„±ëŸ‰ (TPS)
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”

### 1. GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
```bash
# GPU ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì‹¤í–‰
# ì‹¤ì‹œê°„ GPU ì‚¬ìš©ë¥  í™•ì¸
watch -n 1 nvidia-smi

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
while true; do
    echo "$(date): $(nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits)"
    sleep 5
done > gpu_memory.log
```

### 2. vLLM ì„±ëŠ¥ íŠœë‹
```python
# vllm_server.pyì—ì„œ ìµœì í™” ì˜µì…˜
llm = LLM(
    model=model_name,
    tensor_parallel_size=1,
    dtype="half",           # ë©”ëª¨ë¦¬ ì ˆì•½
    max_model_len=2048,     # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì œí•œ
    gpu_memory_utilization=0.8,  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¡°ì •
    swap_space=4,           # ìŠ¤ì™‘ ê³µê°„ (GB)
)
```

### 3. ì„œë²„ ì•ˆì •ì„± í–¥ìƒ
```bash
# Systemd ì„œë¹„ìŠ¤ ìƒì„± (ìë™ ì¬ì‹œì‘)
sudo tee /etc/systemd/system/vllm-server.service << EOF
[Unit]
Description=vLLM Inference Server
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=/home/$USER
Environment=PATH=/home/$USER/vllm-env/bin
ExecStart=/home/$USER/vllm-env/bin/python /home/$USER/vllm_server.py
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
EOF

# ì„œë¹„ìŠ¤ í™œì„±í™”
sudo systemctl enable vllm-server
sudo systemctl start vllm-server
sudo systemctl status vllm-server
```

## ğŸ§ª ë‹¤ì–‘í•œ ëª¨ë¸ í…ŒìŠ¤íŠ¸

### 1. ëª¨ë¸ ë³€ê²½ ë° í…ŒìŠ¤íŠ¸
```python
# ë‹¤ë¥¸ ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸í•´ë³´ê¸°
models_to_test = [
    "microsoft/DialoGPT-medium",      # ëŒ€í™”í˜• (ê°€ë²¼ì›€)
    "microsoft/DialoGPT-large",       # ëŒ€í™”í˜• (ë” í° ëª¨ë¸)
    "codellama/CodeLlama-7b-hf",      # ì½”ë“œ ìƒì„± (7B)
    "mistralai/Mistral-7B-v0.1",      # ì¼ë°˜ ìš©ë„ (7B)
]
```

### 2. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
```bash
# ê° ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ì†ë„ ì¸¡ì •
# 1. ëª¨ë¸ ë³€ê²½
# 2. ì„œë²„ ì¬ì‹œì‘
# 3. ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë²¤ì¹˜ë§ˆí¬
# 4. ê²°ê³¼ ë¹„êµ ë¶„ì„
```

## ğŸ’° ë¹„ìš© ìµœì í™”

### 1. ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
```bash
# ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ì§€
gcloud compute instances stop llm-inference-server --zone=us-central1-a

# í•„ìš”í•  ë•Œ ë‹¤ì‹œ ì‹œì‘
gcloud compute instances start llm-inference-server --zone=us-central1-a

# ì™„ì „ ì‚­ì œ (ì£¼ì˜!)
gcloud compute instances delete llm-inference-server --zone=us-central1-a
```

### 2. ì˜ˆìƒ ë¹„ìš© (ì›” ê¸°ì¤€)
```
GPU ì¸ìŠ¤í„´ìŠ¤ ë¹„ìš© (24ì‹œê°„ ìš´ì˜):
- L4 GPU + g2-standard-4: ~$400-500/ì›”
- T4 GPU + n1-standard-4: ~$300-400/ì›”

ë¹„ìš© ì ˆì•½ íŒ:
- ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ì§€
- Preemptible ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© (50-70% í• ì¸)
- Spot ì¸ìŠ¤í„´ìŠ¤ í™œìš©
- ì ì ˆí•œ GPU íƒ€ì… ì„ íƒ
```

## ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬

### ê°œë°œ ì™„ë£Œ í›„ ì •ë¦¬
```bash
# ì„œë²„ ì¤‘ì§€
gcloud compute instances stop llm-inference-server --zone=us-central1-a

# ë°©í™”ë²½ ê·œì¹™ ì‚­ì œ
gcloud compute firewall-rules delete allow-llm-api
gcloud compute firewall-rules delete allow-llm-monitoring

# ì¸ìŠ¤í„´ìŠ¤ ì‚­ì œ (ì„ íƒ)
gcloud compute instances delete llm-inference-server --zone=us-central1-a
```

## ğŸ“ ì˜¤ëŠ˜ì˜ ì„±ê³¼
- [ ] GPU í• ë‹¹ëŸ‰ ìš”ì²­ ë° ìŠ¹ì¸ ëŒ€ê¸°
- [ ] Deep Learning VM ê¸°ë°˜ GPU ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
- [ ] vLLM ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì • ì™„ë£Œ
- [ ] FastAPI ê¸°ë°˜ ì¶”ë¡  ì„œë²„ êµ¬ì¶•
- [ ] ì™¸ë¶€ ì ‘ê·¼ì„ ìœ„í•œ ë°©í™”ë²½ ì„¤ì •
- [ ] ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ì™€ ì—°ë™ í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] ì„œë²„ ì•ˆì •ì„± ë° ìë™ ì¬ì‹œì‘ êµ¬ì„±

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„
1. **ê³ ê¸‰ ëª¨ë¸ í…ŒìŠ¤íŠ¸**
   - Llama 2/3 ëª¨ë¸ ë¡œë“œ
   - Code Llama, Mistral ë“± íŠ¹í™” ëª¨ë¸
   - ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ

2. **í”„ë¡œë•ì…˜ ìµœì í™”**
   - ë¡œë“œ ë°¸ëŸ°ì‹± êµ¬ì„±
   - ìºì‹± ë ˆì´ì–´ ì¶”ê°€
   - API í‚¤ ì¸ì¦ êµ¬í˜„

3. **ëª¨ë‹ˆí„°ë§ ê³ ë„í™”**
   - Prometheus + Grafana ì—°ë™
   - ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•
   - ë¡œê·¸ ìˆ˜ì§‘ ë° ë¶„ì„

## ğŸ’¡ ì˜¤ëŠ˜ ë°°ìš´ ì 
- GPU í• ë‹¹ëŸ‰ ìš”ì²­ì´ ì‚¬ì „ì— í•„ìš”í•¨
- Deep Learning VMì´ í™˜ê²½ êµ¬ì¶•ì— ë§¤ìš° ìœ ìš©
- vLLMì´ ì¶”ë¡  ì„±ëŠ¥ ìµœì í™”ì— íš¨ê³¼ì 
- GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ê°€ ëª¨ë¸ ì„ íƒì˜ í•µì‹¬
- ë¹„ìš© ê´€ë¦¬ë¥¼ ìœ„í•œ ì¸ìŠ¤í„´ìŠ¤ ì œì–´ê°€ ì¤‘ìš”

## âš ï¸ ì£¼ì˜ì‚¬í•­
- GPU í• ë‹¹ëŸ‰ ìŠ¹ì¸ê¹Œì§€ 24-48ì‹œê°„ ì†Œìš”
- GPU ì¸ìŠ¤í„´ìŠ¤ëŠ” ë†’ì€ ë¹„ìš© ë°œìƒ (ì‹œê°„ë‹¹ $1-2)
- ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ ë°˜ë“œì‹œ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ì§€
- í° ëª¨ë¸ì¼ìˆ˜ë¡ GPU ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ì¦ê°€
- ë°©í™”ë²½ ì„¤ì • ì‹œ ë³´ì•ˆ ê³ ë ¤ì‚¬í•­ í™•ì¸