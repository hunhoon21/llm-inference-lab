# 2025-08-06: EC2 LLM ì„¤ì • ê°€ì´ë“œ ê°œì„  ë° ì‹¤ìš©ì„± ë³´ê°•

## ğŸ“‹ ì˜¤ëŠ˜ì˜ ëª©í‘œ
ì–´ì œ ì‘ì„±í•œ EC2 LLM ì„¤ì • ê°€ì´ë“œë¥¼ ê²€í† í•˜ê³ , ì‹¤ì œ í™˜ê²½ì—ì„œ ìœ ìš©í•œ ì„¸ë¶€ì‚¬í•­ë“¤ì„ ì¶”ê°€í•˜ì—¬ ì‹¤ìš©ì„±ì„ ë†’ì´ê¸°

## ğŸ”§ AWS ì¸í”„ë¼ ì„¤ì • ë³´ê°•

### 1. EC2 ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë¹„êµ ë¶„ì„

**ìƒì„¸ ì¸ìŠ¤í„´ìŠ¤ ë¹„êµí‘œ**
| ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… | GPU | GPU ë©”ëª¨ë¦¬ | vCPU | ë©”ëª¨ë¦¬ | ì‹œê°„ë‹¹ ë¹„ìš© | ì í•©í•œ ëª¨ë¸ í¬ê¸° |
|--------------|-----|-----------|------|--------|-------------|----------------|
| g5.xlarge    | A10G | 24GB      | 4    | 16GB   | $1.006      | 7B~13B |
| g5.2xlarge   | A10G | 24GB      | 8    | 32GB   | $1.212      | 13B~30B |
| g4dn.xlarge  | T4   | 16GB      | 4    | 16GB   | $0.526      | 3B~7B |
| p3.2xlarge   | V100 | 16GB      | 8    | 61GB   | $3.06       | 7B~13B (ê³ ì„±ëŠ¥) |

**ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ ê°€ì´ë“œ**
```bash
# ê°œë°œ/í…ŒìŠ¤íŠ¸ ìš©ë„ (ë¹„ìš© ìš°ì„ )
# g4dn.xlarge ì¶”ì²œ - Llama3-7BëŠ” ì–‘ìí™” í•„ìš”

# ì¼ë°˜ ìš´ì˜ ìš©ë„ (ê· í˜•)
# g5.xlarge ì¶”ì²œ - 24GB GPU ë©”ëª¨ë¦¬ë¡œ ì—¬ìœ ìˆìŒ

# ê³ ì„±ëŠ¥ ì¶”ë¡  ìš©ë„ (ì„±ëŠ¥ ìš°ì„ )  
# p3.2xlarge ì¶”ì²œ - V100ì˜ ë†’ì€ ì—°ì‚° ì„±ëŠ¥
```

### 2. ë¹„ìš© ìµœì í™” ì „ëµ êµ¬ì²´í™”

**ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ í™œìš©**
```bash
# ìŠ¤íŒŸ ê°€ê²© í™•ì¸
aws ec2 describe-spot-price-history \
  --instance-types g5.xlarge \
  --product-descriptions "Linux/UNIX" \
  --max-items 5

# ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ìš”ì²­
aws ec2 request-spot-instances \
  --spot-price "0.30" \
  --instance-count 1 \
  --type "persistent" \
  --launch-specification file://spot-specification.json

# spot-specification.json ì˜ˆì‹œ
{
  "ImageId": "ami-0c02fb55956c7d316",
  "KeyName": "llm-inference-key",
  "SecurityGroups": ["llm-inference-sg"],
  "InstanceType": "g5.xlarge",
  "Placement": {
    "AvailabilityZone": "us-east-1a"
  },
  "BlockDeviceMappings": [
    {
      "DeviceName": "/dev/sda1",
      "Ebs": {
        "VolumeSize": 50,
        "VolumeType": "gp3",
        "DeleteOnTermination": true
      }
    }
  ]
}
```

**ìë™ ì‹œì‘/ì¤‘ì§€ ìŠ¤ì¼€ì¤„ë§**
```bash
# CloudWatch Eventsë¥¼ í†µí•œ ìë™í™”
# í‰ì¼ 9ì‹œ ì‹œì‘, 18ì‹œ ì¤‘ì§€ë¡œ ì•½ 60% ë¹„ìš© ì ˆì•½

# ì‹œì‘ ìŠ¤ì¼€ì¤„ (ì›”-ê¸ˆ 9:00 KST)
aws events put-rule \
  --name "start-llm-server" \
  --schedule-expression "cron(0 0 ? * MON-FRI *)"

# ì¤‘ì§€ ìŠ¤ì¼€ì¤„ (ì›”-ê¸ˆ 18:00 KST)  
aws events put-rule \
  --name "stop-llm-server" \
  --schedule-expression "cron(0 9 ? * MON-FRI *)"
```

### 3. ë„¤íŠ¸ì›Œí¬ ë° ë³´ì•ˆ ê°•í™”

**VPC ê¸°ë°˜ ì„¤ì • (í”„ë¡œë•ì…˜ í™˜ê²½)**
```bash
# VPC ìƒì„±
VPC_ID=$(aws ec2 create-vpc \
  --cidr-block 10.0.0.0/16 \
  --query 'Vpc.VpcId' \
  --output text)

# í¼ë¸”ë¦­ ì„œë¸Œë„· ìƒì„±
SUBNET_ID=$(aws ec2 create-subnet \
  --vpc-id $VPC_ID \
  --cidr-block 10.0.1.0/24 \
  --availability-zone us-east-1a \
  --query 'Subnet.SubnetId' \
  --output text)

# ì¸í„°ë„· ê²Œì´íŠ¸ì›¨ì´ ìƒì„± ë° ì—°ê²°
IGW_ID=$(aws ec2 create-internet-gateway \
  --query 'InternetGateway.InternetGatewayId' \
  --output text)

aws ec2 attach-internet-gateway \
  --vpc-id $VPC_ID \
  --internet-gateway-id $IGW_ID

# ë¼ìš°íŠ¸ í…Œì´ë¸” ì„¤ì •
ROUTE_TABLE_ID=$(aws ec2 create-route-table \
  --vpc-id $VPC_ID \
  --query 'RouteTable.RouteTableId' \
  --output text)

aws ec2 create-route \
  --route-table-id $ROUTE_TABLE_ID \
  --destination-cidr-block 0.0.0.0/0 \
  --gateway-id $IGW_ID

aws ec2 associate-route-table \
  --subnet-id $SUBNET_ID \
  --route-table-id $ROUTE_TABLE_ID
```

**ê°œì„ ëœ ë³´ì•ˆ ê·¸ë£¹**
```bash
# ë³´ì•ˆ ê·¸ë£¹ ìƒì„± (VPC ê¸°ë°˜)
SECURITY_GROUP_ID=$(aws ec2 create-security-group \
  --group-name llm-inference-sg \
  --description "Enhanced security group for LLM service" \
  --vpc-id $VPC_ID \
  --query 'GroupId' \
  --output text)

# SSH ì ‘ê·¼ (íŠ¹ì • IPë§Œ í—ˆìš©)
aws ec2 authorize-security-group-ingress \
  --group-id $SECURITY_GROUP_ID \
  --protocol tcp \
  --port 22 \
  --cidr <YOUR_IP>/32

# API ì ‘ê·¼ (í•„ìš”í•œ ê²½ìš°ë§Œ)
aws ec2 authorize-security-group-ingress \
  --group-id $SECURITY_GROUP_ID \
  --protocol tcp \
  --port 8000 \
  --cidr 10.0.0.0/16  # VPC ë‚´ë¶€ì—ì„œë§Œ ì ‘ê·¼

# HTTPS ì ‘ê·¼ (ALB ì‚¬ìš© ì‹œ)
aws ec2 authorize-security-group-ingress \
  --group-id $SECURITY_GROUP_ID \
  --protocol tcp \
  --port 443 \
  --cidr 0.0.0.0/0
```

## ğŸš€ ì„œë²„ í™˜ê²½ êµ¬ì¶• ê°œì„ ì‚¬í•­

### 1. ìë™í™” ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

**ì™„ì „ ìë™í™”ëœ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸**
```bash
#!/bin/bash
# setup-llm-server.sh - ì›í´ë¦­ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

set -euo pipefail  # ì—„ê²©í•œ ì—ëŸ¬ ì²˜ë¦¬

# ìƒ‰ìƒ ì¶œë ¥ì„ ìœ„í•œ í•¨ìˆ˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# ì¸ìˆ˜ íŒŒì‹±
MODEL_NAME=${1:-"meta-llama/Meta-Llama-3-8B"}
GPU_MEMORY_UTILIZATION=${2:-0.8}
MAX_MODEL_LEN=${3:-4096}

log_info "Installing LLM Inference Server"
log_info "Model: $MODEL_NAME"
log_info "GPU Memory Utilization: $GPU_MEMORY_UTILIZATION"
log_info "Max Model Length: $MAX_MODEL_LEN"

# 1. ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
log_info "Collecting system information..."
echo "OS: $(lsb_release -d | cut -f2)"
echo "Kernel: $(uname -r)"
echo "Architecture: $(arch)"

# 2. NVIDIA ë“œë¼ì´ë²„ í™•ì¸
if ! command -v nvidia-smi &> /dev/null; then
    log_error "NVIDIA drivers not found!"
    log_info "Installing NVIDIA drivers..."
    sudo apt update
    sudo apt install -y nvidia-driver-535
    log_warn "Please reboot the system and run this script again"
    exit 1
fi

log_info "NVIDIA Driver version: $(nvidia-smi --query-gpu=driver_version --format=csv,noheader)"

# 3. ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸
log_info "Updating system packages..."
sudo apt update && sudo apt upgrade -y

# 4. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
log_info "Installing essential packages..."
sudo apt install -y htop nvtop git curl wget python3-pip python3-venv \
                    software-properties-common apt-transport-https ca-certificates

# 5. Python ê°€ìƒí™˜ê²½ ì„¤ì •
log_info "Setting up Python virtual environment..."
python3 -m venv vllm-env
source vllm-env/bin/activate

# 6. Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
log_info "Installing Python packages..."
pip install --upgrade pip setuptools wheel

# CUDA ë²„ì „ í™•ì¸ í›„ PyTorch ì„¤ì¹˜
CUDA_VERSION=$(nvidia-smi | grep -o 'CUDA Version: [0-9.]*' | cut -d' ' -f3 | cut -d'.' -f1,2)
log_info "Detected CUDA version: $CUDA_VERSION"

if [[ $CUDA_VERSION == "11.8" ]]; then
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
elif [[ $CUDA_VERSION == "12.1" ]]; then
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
else
    log_warn "Unsupported CUDA version. Installing CPU version of PyTorch"
    pip install torch torchvision torchaudio
fi

# vLLM ë° ì˜ì¡´ì„± ì„¤ì¹˜
pip install vllm transformers accelerate huggingface_hub

# 7. HuggingFace í† í° í™•ì¸
if [ -z "${HF_TOKEN:-}" ]; then
    log_warn "HF_TOKEN environment variable not set"
    read -p "Enter your HuggingFace token (or press Enter to skip): " token
    if [ -n "$token" ]; then
        echo "export HF_TOKEN='$token'" >> ~/.bashrc
        export HF_TOKEN="$token"
    fi
fi

# 8. ì„œë¹„ìŠ¤ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
log_info "Creating service scripts..."

cat > start-llm-server.sh << EOF
#!/bin/bash
source vllm-env/bin/activate
python -m vllm.entrypoints.openai.api_server \\
  --model $MODEL_NAME \\
  --host 0.0.0.0 \\
  --port 8000 \\
  --gpu-memory-utilization $GPU_MEMORY_UTILIZATION \\
  --max-model-len $MAX_MODEL_LEN \\
  --disable-log-requests \\
  --served-model-name llama3-8b
EOF

chmod +x start-llm-server.sh

cat > stop-llm-server.sh << 'EOF'
#!/bin/bash
pkill -f "vllm.entrypoints.openai.api_server"
echo "LLM server stopped"
EOF

chmod +x stop-llm-server.sh

# 9. Systemd ì„œë¹„ìŠ¤ ìƒì„± (ì„ íƒì‚¬í•­)
cat > llm-inference.service << EOF
[Unit]
Description=LLM Inference Server
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=$HOME
ExecStart=$HOME/start-llm-server.sh
ExecStop=$HOME/stop-llm-server.sh
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

log_info "To install as system service, run:"
log_info "sudo cp llm-inference.service /etc/systemd/system/"
log_info "sudo systemctl enable llm-inference.service"

# 10. ë°©í™”ë²½ ì„¤ì •
log_info "Configuring firewall..."
sudo ufw allow 22/tcp
sudo ufw allow 8000/tcp
echo "y" | sudo ufw enable || true

# 11. ì„¤ì¹˜ ì™„ë£Œ
log_info "Installation completed successfully!"
log_info "To start the server: ./start-llm-server.sh"
log_info "To stop the server: ./stop-llm-server.sh"
log_info "To test the API: curl http://localhost:8000/v1/models"

# 12. ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
log_info "System Summary:"
echo "GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader)"
echo "GPU Memory: $(nvidia-smi --query-gpu=memory.total --format=csv,noheader)"
echo "Python: $(python --version)"
echo "PyTorch: $(python -c 'import torch; print(torch.__version__)')"
echo "CUDA Available: $(python -c 'import torch; print(torch.cuda.is_available())')"
```

### 2. í™˜ê²½ë³„ ì„¤ì • í…œí”Œë¦¿

**ê°œë°œ í™˜ê²½ ì„¤ì •**
```bash
# dev-config.sh
export MODEL_NAME="meta-llama/Meta-Llama-3-8B"
export GPU_MEMORY_UTILIZATION="0.7"
export MAX_MODEL_LEN="2048"
export LOG_LEVEL="INFO"
export DISABLE_LOG_REQUESTS="true"

# ê°œë°œìš© ì¶”ê°€ íŒŒë¼ë¯¸í„°
export TENSOR_PARALLEL_SIZE="1"
export PIPELINE_PARALLEL_SIZE="1"
export QUANTIZATION="awq"  # ë©”ëª¨ë¦¬ ì ˆì•½
```

**í”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì •**
```bash
# prod-config.sh
export MODEL_NAME="meta-llama/Meta-Llama-3-8B"
export GPU_MEMORY_UTILIZATION="0.9"
export MAX_MODEL_LEN="4096"
export LOG_LEVEL="WARNING"
export DISABLE_LOG_REQUESTS="true"

# í”„ë¡œë•ì…˜ ìµœì í™” íŒŒë¼ë¯¸í„°
export TENSOR_PARALLEL_SIZE="1"
export PIPELINE_PARALLEL_SIZE="1"
export ENGINE_USE_RAY="true"
export SWAP_SPACE="4"
```

## ğŸ¤– Llama3-7B ëª¨ë¸ ìµœì í™”

### 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìµœì í™”

**ëŒ€ì—­í­ ìµœì í™” ë‹¤ìš´ë¡œë“œ**
```bash
# ë³‘ë ¬ ë‹¤ìš´ë¡œë“œë¡œ ì†ë„ í–¥ìƒ
export HF_HUB_DOWNLOAD_MAX_WORKERS=4

# ì¬ì‹œì‘ ê°€ëŠ¥í•œ ë‹¤ìš´ë¡œë“œ
huggingface-cli download meta-llama/Meta-Llama-3-8B \
  --cache-dir ./models \
  --resume-download \
  --local-dir-use-symlinks False

# ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥  í™•ì¸
du -sh ~/.cache/huggingface/hub/
```

**ëª¨ë¸ íŒŒì¼ êµ¬ì¡° ì´í•´**
```bash
# ëª¨ë¸ êµ¬ì„± ìš”ì†Œ í™•ì¸
ls -la ~/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/*/

# ì£¼ìš” íŒŒì¼ë“¤:
# - pytorch_model-*.bin (ëª¨ë¸ ê°€ì¤‘ì¹˜)
# - config.json (ëª¨ë¸ ì„¤ì •)
# - tokenizer.json (í† í¬ë‚˜ì´ì €)
# - generation_config.json (ìƒì„± ì„¤ì •)
```

### 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”

**ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ**
```python
# memory_optimizer.py
import torch
import psutil
from vllm import LLM

def get_optimal_batch_size():
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ìµœì  ë°°ì¹˜ í¬ê¸° ê²°ì •"""
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
    system_memory = psutil.virtual_memory().total / (1024**3)  # GB
    
    if gpu_memory >= 24:
        return 64
    elif gpu_memory >= 16:
        return 32
    elif gpu_memory >= 12:
        return 16
    else:
        return 8

def get_optimal_model_len():
    """GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ìµœì  ëª¨ë¸ ê¸¸ì´ ê²°ì •"""
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    
    if gpu_memory >= 24:
        return 8192
    elif gpu_memory >= 16:
        return 4096
    elif gpu_memory >= 12:
        return 2048
    else:
        return 1024

print(f"Recommended batch size: {get_optimal_batch_size()}")
print(f"Recommended max model length: {get_optimal_model_len()}")
```

## ğŸ”¥ vLLM ì„œë¹„ìŠ¤ ê³ ê¸‰ ì„¤ì •

### 1. ë‹¤ì¤‘ ëª¨ë¸ ì„œë¹™

**ëª¨ë¸ ì„œë¹™ ì„¤ì •**
```bash
# ì—¬ëŸ¬ ëª¨ë¸ì„ ë™ì‹œì— ì„œë¹™
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Meta-Llama-3-8B \
  --model microsoft/DialoGPT-medium \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.8
```

### 2. ë¡œë“œë°¸ëŸ°ì‹± ë° ê³ ê°€ìš©ì„±

**Nginx ë¡œë“œë°¸ëŸ°ì„œ ì„¤ì •**
```nginx
# /etc/nginx/sites-available/llm-inference
upstream llm_backend {
    server 127.0.0.1:8000;
    server 127.0.0.1:8001 backup;
}

server {
    listen 80;
    server_name your-domain.com;

    location /v1/ {
        proxy_pass http://llm_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_connect_timeout 300s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;
    }
}
```

### 3. API ì‘ë‹µ ìºì‹±

**Redis ê¸°ë°˜ ìºì‹±**
```python
# api_cache.py
import redis
import hashlib
import json

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def cache_key(prompt, model, **kwargs):
    """ìºì‹œ í‚¤ ìƒì„±"""
    data = {'prompt': prompt, 'model': model, **kwargs}
    return hashlib.md5(json.dumps(data, sort_keys=True).encode()).hexdigest()

def get_cached_response(prompt, model, **kwargs):
    """ìºì‹œëœ ì‘ë‹µ ì¡°íšŒ"""
    key = cache_key(prompt, model, **kwargs)
    cached = redis_client.get(key)
    if cached:
        return json.loads(cached)
    return None

def cache_response(prompt, model, response, ttl=3600, **kwargs):
    """ì‘ë‹µ ìºì‹± (1ì‹œê°„ TTL)"""
    key = cache_key(prompt, model, **kwargs)
    redis_client.setex(key, ttl, json.dumps(response))
```

## ğŸ“Š ê³ ê¸‰ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### 1. Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘

**ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°**
```python
# metrics_collector.py
from prometheus_client import start_http_server, Summary, Counter, Gauge
import time
import nvidia_ml_py3 as nvml
import psutil

# ë©”íŠ¸ë¦­ ì •ì˜
REQUEST_TIME = Summary('llm_request_processing_seconds', 'Time spent processing LLM requests')
REQUEST_COUNT = Counter('llm_requests_total', 'Total LLM requests')
GPU_UTILIZATION = Gauge('gpu_utilization_percent', 'GPU utilization percentage')
GPU_MEMORY_USED = Gauge('gpu_memory_used_mb', 'GPU memory used in MB')

def collect_gpu_metrics():
    """GPU ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
    nvml.nvmlInit()
    handle = nvml.nvmlDeviceGetHandleByIndex(0)
    
    # GPU ì‚¬ìš©ë¥ 
    utilization = nvml.nvmlDeviceGetUtilizationRates(handle)
    GPU_UTILIZATION.set(utilization.gpu)
    
    # GPU ë©”ëª¨ë¦¬
    memory_info = nvml.nvmlDeviceGetMemoryInfo(handle)
    GPU_MEMORY_USED.set(memory_info.used / 1024 / 1024)  # MB

if __name__ == '__main__':
    start_http_server(9090)
    while True:
        collect_gpu_metrics()
        time.sleep(10)
```

### 2. ë¡œê·¸ ë¶„ì„ ë° ì•Œë¦¼

**ë¡œê·¸ íŒŒì‹± ë° ì´ìƒ íƒì§€**
```python
# log_analyzer.py
import re
import smtplib
from email.mime.text import MimeText
from collections import defaultdict, deque
from datetime import datetime, timedelta

class LogAnalyzer:
    def __init__(self):
        self.error_count = defaultdict(int)
        self.recent_errors = deque(maxlen=100)
        self.last_alert = {}
    
    def parse_log_line(self, line):
        """ë¡œê·¸ ë¼ì¸ íŒŒì‹±"""
        patterns = {
            'error': r'ERROR.*',
            'warning': r'WARNING.*',
            'memory_error': r'CUDA out of memory',
            'timeout': r'Request timeout'
        }
        
        for error_type, pattern in patterns.items():
            if re.search(pattern, line):
                self.error_count[error_type] += 1
                self.recent_errors.append({
                    'type': error_type,
                    'message': line,
                    'timestamp': datetime.now()
                })
                
                # ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ ì•Œë¦¼
                if self.should_alert(error_type):
                    self.send_alert(error_type, line)
    
    def should_alert(self, error_type):
        """ì•Œë¦¼ ë°œì†¡ ì—¬ë¶€ ê²°ì •"""
        thresholds = {
            'error': 10,
            'memory_error': 1,
            'timeout': 5
        }
        
        threshold = thresholds.get(error_type, 5)
        recent_count = sum(1 for e in self.recent_errors 
                          if e['type'] == error_type and 
                          e['timestamp'] > datetime.now() - timedelta(minutes=5))
        
        return recent_count >= threshold
    
    def send_alert(self, error_type, message):
        """ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡"""
        # êµ¬í˜„ ìƒëµ
        pass
```

## ğŸ§ª API í…ŒìŠ¤íŠ¸ ê³ ë„í™”

### 1. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë„êµ¬

**ë©€í‹°ìŠ¤ë ˆë“œ ë¶€í•˜ í…ŒìŠ¤íŠ¸**
```python
# load_test.py
import asyncio
import aiohttp
import time
import statistics
from concurrent.futures import ThreadPoolExecutor
import argparse

async def send_request(session, prompt, semaphore):
    """ë‹¨ì¼ ìš”ì²­ ì „ì†¡"""
    async with semaphore:
        try:
            start_time = time.time()
            async with session.post('http://localhost:8000/v1/completions',
                                  json={
                                      'model': 'meta-llama/Meta-Llama-3-8B',
                                      'prompt': prompt,
                                      'max_tokens': 100,
                                      'temperature': 0.7
                                  }) as response:
                result = await response.json()
                end_time = time.time()
                return {
                    'latency': end_time - start_time,
                    'status': response.status,
                    'tokens': len(result.get('choices', [{}])[0].get('text', '').split())
                }
        except Exception as e:
            return {'error': str(e)}

async def run_load_test(concurrent_requests=10, total_requests=100):
    """ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    semaphore = asyncio.Semaphore(concurrent_requests)
    connector = aiohttp.TCPConnector(limit=concurrent_requests*2)
    
    async with aiohttp.ClientSession(connector=connector) as session:
        prompts = [f"Write a story about {i}" for i in range(total_requests)]
        
        start_time = time.time()
        tasks = [send_request(session, prompt, semaphore) for prompt in prompts]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        end_time = time.time()
        
        # ê²°ê³¼ ë¶„ì„
        successful_results = [r for r in results if isinstance(r, dict) and 'latency' in r]
        error_count = len(results) - len(successful_results)
        
        if successful_results:
            latencies = [r['latency'] for r in successful_results]
            tokens_per_request = [r['tokens'] for r in successful_results]
            
            print(f"=== Load Test Results ===")
            print(f"Total requests: {total_requests}")
            print(f"Concurrent requests: {concurrent_requests}")
            print(f"Successful requests: {len(successful_results)}")
            print(f"Failed requests: {error_count}")
            print(f"Total time: {end_time - start_time:.2f}s")
            print(f"Requests per second: {len(successful_results)/(end_time - start_time):.2f}")
            print(f"Average latency: {statistics.mean(latencies):.2f}s")
            print(f"95th percentile latency: {statistics.quantiles(latencies, n=20)[18]:.2f}s")
            print(f"Average tokens per request: {statistics.mean(tokens_per_request):.1f}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--concurrent', type=int, default=10)
    parser.add_argument('--total', type=int, default=100)
    args = parser.parse_args()
    
    asyncio.run(run_load_test(args.concurrent, args.total))
```

### 2. API ì‘ë‹µ í’ˆì§ˆ í…ŒìŠ¤íŠ¸

**ì‘ë‹µ í’ˆì§ˆ ìë™ í‰ê°€**
```python
# quality_test.py
import openai
from textstat import flesch_reading_ease
import re

def evaluate_response_quality(prompt, response):
    """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
    metrics = {}
    
    # 1. ê¸¸ì´ ì ì ˆì„±
    metrics['length'] = len(response.split())
    metrics['length_score'] = 1.0 if 50 <= metrics['length'] <= 200 else 0.5
    
    # 2. ê°€ë…ì„± ì ìˆ˜
    metrics['readability'] = flesch_reading_ease(response)
    metrics['readability_score'] = 1.0 if metrics['readability'] > 60 else 0.5
    
    # 3. ë°˜ë³µì„± ì²´í¬
    words = response.split()
    unique_words = set(words)
    metrics['repetition_ratio'] = len(unique_words) / len(words) if words else 0
    metrics['repetition_score'] = 1.0 if metrics['repetition_ratio'] > 0.7 else 0.5
    
    # 4. ê´€ë ¨ì„± (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
    prompt_words = set(prompt.lower().split())
    response_words = set(response.lower().split())
    common_words = prompt_words.intersection(response_words)
    metrics['relevance_ratio'] = len(common_words) / len(prompt_words) if prompt_words else 0
    metrics['relevance_score'] = 1.0 if metrics['relevance_ratio'] > 0.3 else 0.5
    
    # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
    metrics['overall_score'] = (
        metrics['length_score'] + 
        metrics['readability_score'] + 
        metrics['repetition_score'] + 
        metrics['relevance_score']
    ) / 4
    
    return metrics

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
test_prompts = [
    "Explain artificial intelligence in simple terms",
    "Write a short story about a robot",
    "What are the benefits of renewable energy?"
]

for prompt in test_prompts:
    # API í˜¸ì¶œ (ì‹¤ì œ êµ¬í˜„ í•„ìš”)
    response = "Sample response text here..."
    quality = evaluate_response_quality(prompt, response)
    print(f"Prompt: {prompt}")
    print(f"Quality Score: {quality['overall_score']:.2f}")
    print("---")
```

## ğŸ“ ì˜¤ëŠ˜ì˜ ì„±ê³¼
- [x] EC2 ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ë³„ ìƒì„¸ ë¹„êµ ë¶„ì„ ì™„ë£Œ
- [x] ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ í™œìš©í•œ ë¹„ìš© ìµœì í™” ë°©ì•ˆ ìˆ˜ë¦½
- [x] VPC ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ ê°•í™” ë°©ì•ˆ ì œì‹œ
- [x] ì™„ì „ ìë™í™”ëœ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [x] í™˜ê²½ë³„ ì„¤ì • í…œí”Œë¦¿ ì œê³µ
- [x] ê³ ê¸‰ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ ì„¤ê³„
- [x] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë° í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ë„êµ¬ ê°œë°œ
- [x] ë©€í‹°ëª¨ë¸ ì„œë¹™ ë° ë¡œë“œë°¸ëŸ°ì‹± ë°©ì•ˆ ì œì‹œ

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„ (2025-08-07 ê³„íš)
1. **ì‹¤ì œ í™˜ê²½ í…ŒìŠ¤íŠ¸**: ê°œì„ ëœ ìŠ¤í¬ë¦½íŠ¸ë¡œ EC2 í™˜ê²½ êµ¬ì¶•
2. **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹**: ë‹¤ì–‘í•œ ì„¤ì •ì— ëŒ€í•œ ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜
3. **í´ë¼ì´ì–¸íŠ¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ**: Python/JavaScript í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„
4. **ìš´ì˜ í™˜ê²½ ì¤€ë¹„**: ëª¨ë‹ˆí„°ë§, ë¡œê¹…, ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•
5. **ë¬¸ì„œí™”**: ìš´ì˜ ë§¤ë‰´ì–¼ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ì‘ì„±

## ğŸ’¡ ì£¼ìš” ê°œì„  ì‚¬í•­
- **ì‹¤ìš©ì„±**: ì™„ì „ ìë™í™”ëœ ì„¤ì¹˜ í”„ë¡œì„¸ìŠ¤
- **ì•ˆì •ì„±**: ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ ë§¤ì»¤ë‹ˆì¦˜ ê°•í™”
- **í™•ì¥ì„±**: ë©€í‹° ëª¨ë¸ ì„œë¹™ ë° ë¡œë“œë°¸ëŸ°ì‹± ì§€ì›
- **ëª¨ë‹ˆí„°ë§**: Prometheus ê¸°ë°˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- **í’ˆì§ˆ ê´€ë¦¬**: ìë™í™”ëœ ì‘ë‹µ í’ˆì§ˆ í‰ê°€
- **ë¹„ìš© íš¨ìœ¨ì„±**: ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ë° ìŠ¤ì¼€ì¤„ë§ í™œìš©
