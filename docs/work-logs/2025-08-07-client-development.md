# 2025-08-07: LLM ν΄λΌμ΄μ–ΈνΈ μ• ν”λ¦¬μΌ€μ΄μ… κ°λ°

## π“‹ μ¤λμ λ©ν‘
μ‹¤ν— μΉν™”μ μΈ CLI ν΄λΌμ΄μ–ΈνΈ κ°λ°ν•μ—¬ LLM μ¶”λ΅  μ„λΉ„μ¤μ™€ ν¨μ¨μ μΌλ΅ ν†µμ‹ ν•  μ μλ” λ„κµ¬ μ™„μ„±

## π› οΈ ν΄λΌμ΄μ–ΈνΈ κ°λ°

### 1. ν•µμ‹¬ κΈ°λ¥ κµ¬ν„
**κΈ°λ³Έ API ν†µμ‹ **
```python
class LLMClient:
    def generate_single(self, prompt: str) -> RequestResult
    def generate_batch(self, prompts: List[str], concurrent: int) -> List[RequestResult]
    def health_check(self) -> bool
```

**μ‹¤ν— μ§€μ› κΈ°λ¥**
- λ°°μΉ μ²λ¦¬ (TXT/JSON νμΌ μ§€μ›)
- λ³‘λ ¬ μ”μ²­ μ²λ¦¬
- κ²°κ³Ό μ €μ¥ (JSON/CSV)
- μ„±λ¥ μΈ΅μ • λ° λ¦¬ν¬ν…

### 2. μ‚¬μ© λ¨λ“
```bash
# λ€ν™”ν• λ¨λ“
python llm_client.py --interactive

# λ‹¨μΌ ν”„λ΅¬ν”„νΈ ν…μ¤νΈ
python llm_client.py --prompt "μΈκ³µμ§€λ¥μ„ μ„¤λ…ν•΄μ£Όμ„Έμ”"

# λ°°μΉ μ²λ¦¬
python llm_client.py --prompt-file example_prompts.txt --output results.json

# μ„±λ¥ ν…μ¤νΈ
python llm_client.py --prompt-file prompts.txt --concurrent 3 --repeat 5
```

### 3. νμΌ κµ¬μ΅°
```
client/
β”β”€β”€ llm_client.py           # λ©”μΈ ν΄λΌμ΄μ–ΈνΈ (350 λΌμΈ)
β”β”€β”€ requirements.txt        # requests>=2.28.0
β”β”€β”€ README.md              # μ‚¬μ© κ°€μ΄λ“
β”β”€β”€ example_prompts.txt    # ν•κΈ€ μμ  ν”„λ΅¬ν”„νΈ
β””β”€β”€ example_prompts.json   # JSON ν•μ‹ μμ 
```

## π§ μ‹¤ν— κΈ°λ¥

### 1. μ„±λ¥ μΈ΅μ •
- λ μ΄ν„΄μ‹ μ¶”μ 
- ν† ν° μƒμ„± μ†λ„ κ³„μ‚°
- μ²λ¦¬λ‰(RPS) μΈ΅μ •
- μ„±κ³µ/μ‹¤ν¨μ¨ ν†µκ³„

### 2. κ²°κ³Ό λ¶„μ„
```
π“ EXPERIMENT SUMMARY
==================================================
Total requests: 10
Successful: 9
Failed: 1

β±οΈ  Latency:
  Average: 2.34s
  Min: 1.12s  
  Max: 4.56s

π€ Performance:
  Requests per second: 0.43
  Tokens per second: 37.2
```

### 3. λ‹¤μ–‘ν• μ‹¤ν— μ‹λ‚λ¦¬μ¤
```bash
# μΌκ΄€μ„± ν…μ¤νΈ
python llm_client.py --prompt "2+2λ”?" --repeat 10 --temperature 0.0

# ν’μ§ λΉ„κµ
python llm_client.py --prompt-file prompts.txt --temperature 0.1 --output low_temp.json
python llm_client.py --prompt-file prompts.txt --temperature 0.9 --output high_temp.json

# μ„±λ¥ λ²¤μΉλ§ν‚Ή
python llm_client.py --prompt-file large_dataset.txt --concurrent 5 --output benchmark.csv
```

## π“ μμ  ν”„λ΅¬ν”„νΈ
ν•κΈ€λ΅ μ‘μ„±λ λ‹¤μ–‘ν• μ£Όμ μ ν”„λ΅¬ν”„νΈ 10κ°:
- μΈκ³µμ§€λ¥ μ„¤λ…
- μ‹ μ¬μƒ μ—λ„μ§€
- λ΅λ΄‡ κ°μ • μ΄μ•ΌκΈ°
- λ¨Έμ‹ λ¬λ‹ μ›λ¦¬
- κ΄‘ν•©μ„± κ³Όμ •
- μ°μ£Ό νƒμ‚¬ λ―Έλ
- μ–‘μμ»΄ν“¨ν… μ…λ¬Έ
- AI μ¤λ¦¬
- μ‹ κ²½λ§ ν•™μµ
- κΈ°ν›„λ³€ν™” μν–¥

## π”§ κΈ°μ μ  νΉμ§•

### μ„¤κ³„ μ›μΉ™
- **λ‹¨μν•¨**: requestsλ§ μ‚¬μ©, μ§κ΄€μ  CLI
- **μ‹¤ν— μΉν™”μ **: λ°°μΉ μ²λ¦¬, κ²°κ³Ό μ €μ¥, μ„±λ¥ μΈ΅μ •
- **ν™•μ¥μ„±**: λ¨λ“ν™”λ κµ¬μ΅°, MLflow μ—°λ™ μ¤€λΉ„
- **μ‹ λΆ°μ„±**: κ°•λ ¥ν• μ—λ¬ μ²λ¦¬, νƒ€μ„μ•„μ›ƒ κ΄€λ¦¬

### OpenAI νΈν™ API
- `/v1/completions` μ—”λ“ν¬μΈνΈ μ‚¬μ©
- `/v1/models` λ¨λΈ λ©λ΅ μ΅°ν
- ν‘μ¤€ νλΌλ―Έν„° μ§€μ› (temperature, max_tokens λ“±)

## π“ μ¤λμ μ„±κ³Ό
- [x] μ‹¤ν— μΉν™”μ  CLI ν΄λΌμ΄μ–ΈνΈ μ™„μ„±
- [x] λ°°μΉ μ²λ¦¬ λ° λ³‘λ ¬ μ”μ²­ μ§€μ›
- [x] JSON/CSV κ²°κ³Ό μ €μ¥ κΈ°λ¥
- [x] μ„±λ¥ μΈ΅μ • λ° λ¦¬ν¬ν… μ‹μ¤ν…
- [x] λ€ν™”ν• λ¨λ“ κµ¬ν„
- [x] ν•κΈ€ μμ  ν”„λ΅¬ν”„νΈ μ‘μ„±
- [x] μ‚¬μ© κ°€μ΄λ“ λ¬Έμ„ν™”

## π”„ λ‹¤μ λ‹¨κ³„ (λ‚΄μΌ κ³„ν)
1. EC2μ—μ„ vLLM μ„λ²„μ™€ μ—°λ™ ν…μ¤νΈ
2. μ„±λ¥ λ²¤μΉλ§ν‚Ή μ‹¤ν— μ‹¤ν–‰
3. MLflow μ—°λ™ κ²€ν†  λ° κµ¬ν„
4. μ‘λ‹µ ν’μ§ ν‰κ°€ κΈ°λ¥ μ¶”κ°€

---
**μ‘μ—… μ‹κ°„**: 2025-08-07 9:00 - 12:00  
**μ†μ” μ‹κ°„**: 3μ‹κ°„  
**λΉ„μ©**: $0 (λ΅μ»¬ κ°λ°)