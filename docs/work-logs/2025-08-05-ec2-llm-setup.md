# 2025-08-05: EC2ì— LLM ì¶”ë¡  ì„œë¹„ìŠ¤ êµ¬ì¶•

## ğŸ“‹ ì˜¤ëŠ˜ì˜ ëª©í‘œ
EC2 ì¸ìŠ¤í„´ìŠ¤ì— Llama3-7B + vLLM ì¡°í•©ìœ¼ë¡œ LLM ì¶”ë¡  ì„œë¹„ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³ , HTTP APIë¥¼ í†µí•´ ì¶”ë¡  ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í™˜ê²½ ì™„ì„±

## ğŸ”§ AWS ì¸í”„ë¼ ì„¤ì •

### 1. EC2 ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •
**ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì„ íƒ**
- **ì¶”ì²œ**: `g5.xlarge` (NVIDIA A10G GPU, 4 vCPU, 16GB RAM)
  - ë¹„ìš©: ~$1.006/ì‹œê°„
  - Llama3-7Bì— ì í•©í•œ 16GB GPU ë©”ëª¨ë¦¬
- **ëŒ€ì•ˆ**: `g5.2xlarge` (24GB GPU ë©”ëª¨ë¦¬) - ë” ì—¬ìœ ìˆì§€ë§Œ ë¹„ìš© 2ë°°

**AMI ì„ íƒ**
```bash
# Deep Learning AMI (Ubuntu 20.04) ì‚¬ìš© - CUDA/Python í™˜ê²½ ì‚¬ì „ êµ¬ì„±
# AMI ID: ami-0c02fb55956c7d316 (us-east-1 ê¸°ì¤€)
```

**ìŠ¤í† ë¦¬ì§€ ì„¤ì •**
- EBS ë³¼ë¥¨: 50GB (gp3)
  - Llama3-7B ëª¨ë¸: ~13GB
  - vLLM + ì˜ì¡´ì„±: ~10GB
  - ì‹œìŠ¤í…œ + ì—¬ìœ ê³µê°„: ~27GB

### 2. ë³´ì•ˆ ê·¸ë£¹ ì„¤ì •
```bash
# ë³´ì•ˆ ê·¸ë£¹ ìƒì„±
aws ec2 create-security-group \
  --group-name llm-inference-sg \
  --description "Security group for LLM inference service"

# SSH ì ‘ê·¼ í—ˆìš© (22ë²ˆ í¬íŠ¸)
aws ec2 authorize-security-group-ingress \
  --group-name llm-inference-sg \
  --protocol tcp \
  --port 22 \
  --cidr 0.0.0.0/0

# vLLM API ì ‘ê·¼ í—ˆìš© (8000ë²ˆ í¬íŠ¸)
aws ec2 authorize-security-group-ingress \
  --group-name llm-inference-sg \
  --protocol tcp \
  --port 8000 \
  --cidr 0.0.0.0/0
```

### 3. í‚¤ í˜ì–´ ìƒì„±
```bash
# í‚¤ í˜ì–´ ìƒì„±
aws ec2 create-key-pair \
  --key-name llm-inference-key \
  --query 'KeyMaterial' \
  --output text > llm-inference-key.pem

# ê¶Œí•œ ì„¤ì •
chmod 400 llm-inference-key.pem
```

### 4. EC2 ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘
```bash
# ì¸ìŠ¤í„´ìŠ¤ ì‹¤í–‰
aws ec2 run-instances \
  --image-id ami-0c02fb55956c7d316 \
  --count 1 \
  --instance-type g5.xlarge \
  --key-name llm-inference-key \
  --security-groups llm-inference-sg \
  --block-device-mappings '[{"DeviceName":"/dev/sda1","Ebs":{"VolumeSize":50,"VolumeType":"gp3"}}]' \
  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=llm-inference-server}]'
```

## ğŸš€ ì„œë²„ í™˜ê²½ êµ¬ì¶•

### 1. EC2 ì¸ìŠ¤í„´ìŠ¤ ì ‘ì†
```bash
# í¼ë¸”ë¦­ IP í™•ì¸ í›„ ì ‘ì†
ssh -i llm-inference-key.pem ubuntu@<PUBLIC_IP>
```

### 2. ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ë° ê¸°ë³¸ ì„¤ì •
```bash
# ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸
sudo apt update && sudo apt upgrade -y

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
sudo apt install -y htop nvtop git curl wget

# NVIDIA ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi
```

### 3. Python í™˜ê²½ ì„¤ì •
```bash
# Conda í™˜ê²½ ìƒì„± (ì´ë¯¸ ì„¤ì¹˜ëœ í™˜ê²½ í™œìš©)
conda create -n vllm python=3.10 -y
conda activate vllm

# ë˜ëŠ” venv ì‚¬ìš©
python3.10 -m venv vllm-env
source vllm-env/bin/activate
```

### 4. vLLM ì„¤ì¹˜
```bash
# CUDA 11.8 ë²„ì „ìš© PyTorch ì„¤ì¹˜
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# vLLM ì„¤ì¹˜
pip install vllm

# ì¶”ê°€ ì˜ì¡´ì„±
pip install transformers accelerate
```

## ğŸ¤– Llama3-7B ëª¨ë¸ ì¤€ë¹„

### 1. HuggingFace í† í° ì„¤ì •
```bash
# HuggingFace CLI ì„¤ì¹˜
pip install huggingface_hub

# í† í° ì„¤ì • (https://huggingface.co/settings/tokens ì—ì„œ ë°œê¸‰)
huggingface-cli login
# ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •
export HF_TOKEN="your_token_here"
```

### 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì„ íƒì‚¬í•­)
```bash
# ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ (vLLMì´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ë¯€ë¡œ ì„ íƒì‚¬í•­)
huggingface-cli download meta-llama/Meta-Llama-3-8B --cache-dir ./models
```

## ğŸ”¥ vLLM ì„œë¹„ìŠ¤ ì‹¤í–‰

### 1. vLLM ì„œë²„ ì‹œì‘
```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì„œë²„ ì‹œì‘
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Meta-Llama-3-8B \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.8 \
  --max-model-len 4096

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (ì„ íƒì‚¬í•­)
nohup python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Meta-Llama-3-8B \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.8 \
  --max-model-len 4096 > vllm.log 2>&1 &
```

### 2. ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
```bash
# í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep vllm

# GPU ì‚¬ìš©ëŸ‰ í™•ì¸
nvidia-smi

# í¬íŠ¸ í™•ì¸
netstat -tlnp | grep 8000

# API í…ŒìŠ¤íŠ¸
curl http://localhost:8000/v1/models
```

## ğŸ§ª API í…ŒìŠ¤íŠ¸

### 1. ë¡œì»¬ í…ŒìŠ¤íŠ¸
```bash
# ì™„ì„±ë„ í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/v1/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Meta-Llama-3-8B",
    "prompt": "The capital of France is",
    "max_tokens": 50,
    "temperature": 0.7
  }'
```

### 2. ì™¸ë¶€ ì ‘ê·¼ í…ŒìŠ¤íŠ¸
```bash
# ë¡œì»¬ ë¨¸ì‹ ì—ì„œ í…ŒìŠ¤íŠ¸
curl -X POST "http://<EC2_PUBLIC_IP>:8000/v1/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Meta-Llama-3-8B",
    "prompt": "Hello, how are you?",
    "max_tokens": 100
  }'
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ì„¤ì •

### 1. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
```bash
# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > monitor.sh << 'EOF'
#!/bin/bash
while true; do
    echo "=== $(date) ==="
    echo "GPU Usage:"
    nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits
    echo "CPU/Memory:"
    top -bn1 | grep "Cpu(s)" | head -1
    free -h | grep Mem
    echo "Disk Usage:"
    df -h / | tail -1
    echo "vLLM Process:"
    ps aux | grep vllm | grep -v grep
    echo "========================"
    sleep 60
done
EOF

chmod +x monitor.sh
# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
./monitor.sh > monitoring.log 2>&1 &
```

### 2. ë¡œê·¸ ìˆ˜ì§‘
```bash
# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/logs

# vLLM ë¡œê·¸ í™•ì¸
tail -f vllm.log

# ì‹œìŠ¤í…œ ë¡œê·¸ í™•ì¸
tail -f /var/log/syslog
```

## ğŸ¯ ì„±ëŠ¥ íŠœë‹ ì˜µì…˜

### 1. vLLM ìµœì í™” íŒŒë¼ë¯¸í„°
```bash
# ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì‹¤í–‰
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Meta-Llama-3-8B \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.9 \
  --max-model-len 2048 \
  --swap-space 4 \
  --disable-log-requests
```

### 2. ì‹œìŠ¤í…œ ìµœì í™”
```bash
# GPU í´ëŸ­ ìµœì í™”
sudo nvidia-smi -pm 1
sudo nvidia-smi -ac 1215,1410

# CPU ê±°ë²„ë„ˆ ì„¤ì •
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
```

## ğŸš¨ ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤
1. **CUDA Out of Memory**: `--gpu-memory-utilization` ê°’ì„ ë‚®ì¶”ê¸° (0.7 â†’ 0.6)
2. **ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨**: HF_TOKEN ì„¤ì • í™•ì¸, ë„¤íŠ¸ì›Œí¬ ìƒíƒœ ì ê²€
3. **í¬íŠ¸ ì ‘ê·¼ ë¶ˆê°€**: ë³´ì•ˆ ê·¸ë£¹ 8000ë²ˆ í¬íŠ¸ ì˜¤í”ˆ í™•ì¸
4. **vLLM ì‹œì‘ ì‹¤íŒ¨**: CUDA ë²„ì „ í˜¸í™˜ì„± í™•ì¸, PyTorch ì¬ì„¤ì¹˜

### ë””ë²„ê¹… ëª…ë ¹ì–´
```bash
# ìƒì„¸ ë¡œê·¸ë¡œ ì‹¤í–‰
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Meta-Llama-3-8B \
  --host 0.0.0.0 \
  --port 8000 \
  --log-level debug

# í™˜ê²½ ì •ë³´ í™•ì¸
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, Version: {torch.version.cuda}')"
python -c "import vllm; print(f'vLLM version: {vllm.__version__}')"
```

## ğŸ“ ì˜¤ëŠ˜ì˜ ì„±ê³¼
- [ ] EC2 g5.xlarge ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì„¤ì •
- [ ] ë³´ì•ˆ ê·¸ë£¹ ë° ë„¤íŠ¸ì›Œí¬ êµ¬ì„±
- [ ] vLLM í™˜ê²½ ì„¤ì¹˜ ë° ì„¤ì •
- [ ] Llama3-7B ëª¨ë¸ ë¡œë“œ ë° ì„œë¹„ìŠ¤ ì‹œì‘
- [ ] HTTP API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- [ ] ê¸°ë³¸ ëª¨ë‹ˆí„°ë§ í™˜ê²½ êµ¬ì¶•

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„ (ë‚´ì¼ ê³„íš)
1. í´ë¼ì´ì–¸íŠ¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ
2. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë° ìµœì í™”
3. ì—ëŸ¬ í•¸ë“¤ë§ ë° ë¡œê¹… ê°œì„ 
4. ë¹„ìš© ë¶„ì„ ë° ìµœì í™” ë°©ì•ˆ ê²€í† 